{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Exercice 1 : Limiter les données\n",
        "\n",
        "a) Supprimer certaines colonnes inutiles des datasets \"iris.csv\" et \"titanic_train.csv\".\n",
        "b) Supprimer les enregistrements contenant des valeurs manquantes.\n",
        "\n"
      ],
      "metadata": {
        "id": "9Bk0y83FFSyj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J7EE6uYiFITT"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Charger le dataset iris.csv\n",
        "iris_df = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/data/iris.csv\")\n",
        "\n",
        "# Charger le dataset titanic_train.csv\n",
        "titanic_df = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/data/titanic_train.csv\")\n",
        "\n",
        "# Supprimer certaines colonnes inutiles des datasets\n",
        "iris_df_filtered = iris_df.drop(columns=['sepal_width'])\n",
        "titanic_df_filtered = titanic_df.drop(columns=['Cabin'])\n",
        "\n",
        "# Supprimer les enregistrements contenant des valeurs manquantes\n",
        "iris_df_filtered.dropna(inplace=True)\n",
        "titanic_df_filtered.dropna(inplace=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "EJSv8QGbWT7E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exercice 2 : Séparer les datasets\n",
        "\n",
        "a) Diviser les datasets \"iris.csv\" et \"titanic_train.csv\" en ensembles d'entraînement et de test, avec une proportion de 80% pour l'entraînement et 20% pour le test."
      ],
      "metadata": {
        "id": "OqLN57LKGMTr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Pour le dataset iris.csv\n",
        "iris_train, iris_test = train_test_split(iris_df, test_size=0.2, random_state=42)\n",
        "\n",
        "# Pour le dataset titanic_train.csv\n",
        "titanic_train, titanic_test = train_test_split(titanic_df, test_size=0.2, random_state=42)\n"
      ],
      "metadata": {
        "id": "sDCoOwijGNp4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ce code divise chaque ensemble de données en un ensemble d'entraînement et un ensemble de test à l'aide de la fonction `train_test_split` de scikit-learn. Les données sont divisées de manière aléatoire en utilisant `random_state=42` pour garantir la reproductibilité des résultats.\n",
        "\n"
      ],
      "metadata": {
        "id": "EiHX6KBPWgjL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exercice 3 : Traitement des données manquantes\n",
        "\n",
        "a) Imputer les données manquantes dans les datasets \"iris.csv\" et \"titanic_train.csv\" en utilisant la moyenne pour les valeurs numériques et la valeur la plus fréquente pour les catégoriques.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "0famxbPWIYAZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "# Pour les données numériques dans le dataset iris.csv\n",
        "numeric_imputer = SimpleImputer(strategy='mean')\n",
        "iris_train[['sepal_length']] = numeric_imputer.fit_transform(iris_train[['sepal_length']])\n",
        "iris_test[['sepal_length']] = numeric_imputer.transform(iris_test[['sepal_length']])\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "# Pour les données catégoriques dans le dataset titanic_train.csv\n",
        "categorical_imputer = SimpleImputer(strategy='most_frequent')\n",
        "titanic_train[['Embarked']] = categorical_imputer.fit_transform(titanic_train[['Embarked']])\n",
        "titanic_test[['Embarked']] = categorical_imputer.transform(titanic_test[['Embarked']])\n"
      ],
      "metadata": {
        "id": "GYEPU9zRIbqf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ce code utilise la classe `SimpleImputer` du module `sklearn.impute` pour remplacer les valeurs manquantes dans les ensembles de données.\n"
      ],
      "metadata": {
        "id": "XdfALHORXRfS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exercice 4 : Préparation des attributs numériques\n",
        "\n",
        "a) Valider sémantiquement les données numériques dans les datasets \"iris.csv\" et \"titanic_train.csv\".\n",
        "b) Réaliser une analyse statistique des données numériques."
      ],
      "metadata": {
        "id": "xe6gci8cIiJm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Validation sémantique des données numériques\n",
        "# (Effectuer des contrôles pour vérifier si les données se situent dans des plages valides)\n",
        "\n",
        "# Analyse statistique des données numériques\n",
        "print(\"Analyse statistique des données numériques dans le dataset iris.csv : \")\n",
        "print(iris_df.describe())\n",
        "\n",
        "print(\"\\nAnalyse statistique des données numériques dans le dataset titanic_train.csv : \")\n",
        "print(titanic_df.describe())\n"
      ],
      "metadata": {
        "id": "DOQGVdz_IkER",
        "outputId": "848804db-99a6-48a8-b34b-67f2a87c6aa5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Analyse statistique des données numériques dans le dataset iris.csv : \n",
            "       sepal_length  sepal_width  petal_length  petal_width\n",
            "count    150.000000   150.000000    150.000000   150.000000\n",
            "mean       5.843333     3.054000      3.758667     1.198667\n",
            "std        0.828066     0.433594      1.764420     0.763161\n",
            "min        4.300000     2.000000      1.000000     0.100000\n",
            "25%        5.100000     2.800000      1.600000     0.300000\n",
            "50%        5.800000     3.000000      4.350000     1.300000\n",
            "75%        6.400000     3.300000      5.100000     1.800000\n",
            "max        7.900000     4.400000      6.900000     2.500000\n",
            "\n",
            "Analyse statistique des données numériques dans le dataset titanic_train.csv : \n",
            "       PassengerId    Survived      Pclass         Age       SibSp  \\\n",
            "count   891.000000  891.000000  891.000000  714.000000  891.000000   \n",
            "mean    446.000000    0.383838    2.308642   29.699118    0.523008   \n",
            "std     257.353842    0.486592    0.836071   14.526497    1.102743   \n",
            "min       1.000000    0.000000    1.000000    0.420000    0.000000   \n",
            "25%     223.500000    0.000000    2.000000   20.125000    0.000000   \n",
            "50%     446.000000    0.000000    3.000000   28.000000    0.000000   \n",
            "75%     668.500000    1.000000    3.000000   38.000000    1.000000   \n",
            "max     891.000000    1.000000    3.000000   80.000000    8.000000   \n",
            "\n",
            "            Parch        Fare  \n",
            "count  891.000000  891.000000  \n",
            "mean     0.381594   32.204208  \n",
            "std      0.806057   49.693429  \n",
            "min      0.000000    0.000000  \n",
            "25%      0.000000    7.910400  \n",
            "50%      0.000000   14.454200  \n",
            "75%      0.000000   31.000000  \n",
            "max      6.000000  512.329200  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ce code effectue une validation sémantique des données numériques en affichant une analyse statistique des deux ensembles de données (`iris_df` et `titanic_df`).\n",
        "\n"
      ],
      "metadata": {
        "id": "-mUlA7CBFRHi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exercice 5 : Feature engineering\n",
        "\n",
        "a) Créer de nouvelles caractéristiques à partir des caractéristiques existantes dans les datasets \"iris.csv\" et \"titanic_train.csv\"."
      ],
      "metadata": {
        "id": "4z2_wxhgIrSW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Exemple de feature engineering pour le dataset iris.csv\n",
        "iris_df['sepal_area_cm2'] = iris_df['sepal_length'] * iris_df['sepal_width']\n",
        "# Exemple de feature engineering pour le dataset titanic_train.csv\n",
        "titanic_df['family_size'] = titanic_df['SibSp'] + titanic_df['Parch'] + 1\n"
      ],
      "metadata": {
        "id": "S3rcm6PtIsYU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "UTs9Tn4RYNOk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exercice 6 : Discrétisation\n",
        "\n",
        "a) Discrétiser les attributs numériques des datasets \"iris.csv\" et \"titanic_train.csv\" en intervalles égaux.\n",
        "\n"
      ],
      "metadata": {
        "id": "V5eKcAt9IzKw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import KBinsDiscretizer\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "# Imputer les valeurs manquantes dans le dataset titanic_train.csv\n",
        "imputer = SimpleImputer(strategy='median')\n",
        "titanic_train_imputed = imputer.fit_transform(titanic_train[['Age', 'Fare']])\n",
        "\n",
        "# Discrétisation des données numériques dans le dataset titanic_train.csv\n",
        "discretizer = KBinsDiscretizer(n_bins=3, encode='ordinal', strategy='uniform')\n",
        "titanic_train_discretized = discretizer.fit_transform(titanic_train_imputed)\n",
        "\n"
      ],
      "metadata": {
        "id": "K60vYDDjI0Ut"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "0aD_oEkOZDtL"
      }
    }
  ]
}